{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Sparseloop Tutorial - 02 - Matrix Multiply\n",
    "\n",
    "This notebook contains a series of examples of a **matrix multiply** computation. The **fibertree** emulator is used to illustrate the impact of a set of optimizations to exploit sparsity. The basic computation is represented by the Einsum:\n",
    "\n",
    "$$ Z_{m,n} = A_{m,k} \\times B_{k,n} $$\n",
    "\n",
    "Note that while the output is nominally a sparse rank-2 tensor, in this notebook the output is assumed to be in an uncompressed format and is directly referenced by **coordinate** since **position** == **coordinate** in an uncompressed format.\n",
    "\n",
    "First, include some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Run boilerplate code to set up environment\n",
    "\n",
    "%run ./prelude.py --style=tree --animation=movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure two rank-2 input tensors\n",
    "\n",
    "The following cell sets up the control sliders to specify the attributes of the `A` and `B` input tensors. Those attributes include their **shape**, which specifies the allowable range of **coordinates** of elements of the tensor and their **density**.\n",
    "\n",
    "The rank names use the following convention:\n",
    "\n",
    "- M - uncontracted dimension in input `A`\n",
    "- N - uncontracted dimension in input `B`\n",
    "- K - contracted dimension shared by `A` and `B`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set default problem instance attributes (i.e., the shape of the tensors)\n",
    "#\n",
    "FORMAT_CYCLES = 2\n",
    "FORMAT_HBM_CHANNELS = 2\n",
    "K = 4\n",
    "I = 4\n",
    "J = 2\n",
    "\n",
    "#\n",
    "# Create controls to configure the `A` and `B` tensors\n",
    "#\n",
    "tm2 = TensorMaker(\"sparseloop-matrix-multiply\", autoload=True)\n",
    "\n",
    "tm2.addTensor(\"A\", rank_ids=[\"FMT_CYCLES\", \"FMT_HBM_CHAN\", \"I\", \"K\"], shape=[FORMAT_CYCLES,FORMAT_HBM_CHANNELS,I, K], density=0.4, color=\"blue\")\n",
    "tm2.addTensor(\"B\", rank_ids=[\"FMT_CYCLES\", \"FMT_HBM_CHAN\", \"K\", \"J\"], shape=[FORMAT_CYCLES, FORMAT_HBM_CHANNELS, K, J], density=0.5, color=\"green\")\n",
    "\n",
    "tm2.displayControls()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and display the input tensors\n",
    "\n",
    "Create the `A` and `B` tensors based on the current settings of the configuration sliders above and display the resulting tensors. These tensors are represented in the **fibertree** tensor abstraction, where for sparse fibers only the **elements** (**coordinate**/**payload** tuples) in a fiber with **non-empty** (non-zero) payloads need be shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Understand how format channel width is determined - max vectorized read width?\n",
    "* Capture encoding process\n",
    "* Capture the relationship between formatted and unformatted tensor\n",
    "* Capture all of the below\n",
    "* Capture a dense represntation of the format\n",
    "\n",
    "## Details:\n",
    "* The number of C2SR channels (FMT_HBM_CHAN) corresponds to the number of HBM banks\n",
    "* Each ordered pair of *(C2SR cycle, C2SR channel)* is associated with a subtensor \"tile\" containing nonzero elements of the underlying tensor.\n",
    "* Tile size (cardinality) is always less than or equal to the HBM bank vectorized access *width* (which determines the \"width\" of C2SR \"format channels\")\n",
    "* Each HBM bank has an effective \"depth\" corresponding to how many vectorized writes' worth of data could fill the bank. This corresponds to the number of C2SR \"cycles\" (FMT_CYCLES)\n",
    "* Each format channel coordinate *n* (HBM bank index *n*) is associated with a unique row parity. (i.e. even/odd for 2 channels, *k mod N* for *N* channels)\n",
    "* All tiles associated with channel *n* store nonzero elements only from rows that match the row parity associated with channel *n*\n",
    "* From a packing perspective, the nonzero elements of the tensor ranks are packed with 100% density into the shallowest vector slots of the HBMs (i.e. the *format* has 100% density in the lowest cycle coordinates; of course the underlying tensor is sparse).\n",
    "* For a given format channel with parity *k*, if the total data size of the *k*-parity rows is less than the bank size, then all HBM vector slots past a certain depth will be zero (i.e. the format channel will be empty for higher cycle coordinates of the format)\n",
    "* If the data size of the parity-*k* rows is not an integer multiple of the vectorized access width, then the deepest populated HBM vector slot may be partially-filled\n",
    "* The aggregate sparsity of the parity-*k* rows may differ for different *k*; therefore some HBM banks will be filled to a greater depth\n",
    "* This means that some cycles of the C2SR representation will have a mixture of empty format channels, <100% dense format channels, and 100% dense format channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_CYCH_IK = tm2.makeTensor(\"A\")\n",
    "B_CYCH_KJ = tm2.makeTensor(\"B\")\n",
    "\n",
    "displayTensor(A_CYCH_IK)\n",
    "displayTensor(B_CYCH_KJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse A loader (spAL) \n",
    "\n",
    "In a strictly taxonomic sense, a MatRaptor PE uses a roughly weight-stationary dataflow with permutation JKI \n",
    "\n",
    "The MatRaptor assigns \n",
    "\n",
    "As a simple illustration of the traversal of a rank-2 tensor, the following code shows how that can be accomplished with two nested `for` loops where the **payload** of the traversal of the top rank is a reference to a **fiber** in the next rank.\n",
    "\n",
    "This natural traversal order where the bottom rank is traversed fastest (i.e., in a depth first fashion) is referred to as a **concordant** traversal and tend to be the most efficient. Other, less efficient, traversal orders are referred to as **discordant**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create a rank-2 tensor\n",
    "#\n",
    "A_CYCH_IK = tm2.makeTensor(\"A\")\n",
    "\n",
    "#uncompressTensor(A_CYCH_MK)\n",
    "\n",
    "#\n",
    "# Get root of tensor\n",
    "#\n",
    "a_cy = A_CYCH_IK.getRoot()\n",
    "\n",
    "#\n",
    "# Animation bookkeeeping\n",
    "#\n",
    "canvas = createCanvas(A_CYCH_IK)\n",
    "\n",
    "cycle = 0\n",
    "\n",
    "for cy, a_ch in a_cy:\n",
    "    #\n",
    "    # Iterate over all row indices\n",
    "    #\n",
    "    for idx in range(I):\n",
    "        cycle = 0\n",
    "        #\n",
    "        # For each nonempty channel\n",
    "        #\n",
    "        for ch, a_i in a_ch:       \n",
    "            # Channel parity match\n",
    "            if ch == (idx % FORMAT_HBM_CHANNELS):\n",
    "                #\n",
    "                # Traverse non-empty elements of top rank\n",
    "                #\n",
    "                for m, a_k in a_i:   \n",
    "                    # Row match\n",
    "                    if m == idx:\n",
    "                        #\n",
    "                        # Traverse non-empty element of bottom rank\n",
    "                        #\n",
    "                        for k, a_val in a_k:  \n",
    "\n",
    "                            print(f\"{a_val} \", end='')\n",
    "\n",
    "                            #\n",
    "                            # Animation bookkeeping\n",
    "                            #\n",
    "                            canvas.addActivity((cy,ch,idx,k), worker=f\"PE{idx}\")\n",
    "                            cycle += 1\n",
    "    canvas.addFrame()\n",
    "\n",
    "displayCanvas(canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create a rank-2 tensor\n",
    "#\n",
    "A_CYCH_MK = tm2.makeTensor(\"A\")\n",
    "\n",
    "#uncompressTensor(A_CYCH_MK)\n",
    "\n",
    "#\n",
    "# Get root of tensor\n",
    "#\n",
    "a_cy = A_CYCH_MK.getRoot()\n",
    "\n",
    "#\n",
    "# Animation bookkeeeping\n",
    "#\n",
    "canvas = createCanvas(A_CYCH_MK)\n",
    "\n",
    "cycle = 0\n",
    "\n",
    "for cy, a_ch in a_cy:\n",
    "    uncompressTensor(a_ch)\n",
    "    #\n",
    "    # Iterate over all row indices\n",
    "    #\n",
    "    for m in range(M):\n",
    "        cycle = 0\n",
    "        #\n",
    "        # For each nonempty channel\n",
    "        #\n",
    "        for ch, a_m in a_ch:       \n",
    "            # Channel parity match\n",
    "            if ch == (m % FORMAT_HBM_CHANNELS):\n",
    "                print(str(ch))\n",
    "                #\n",
    "                # Traverse non-empty elements of top rank\n",
    "                #\n",
    "                for midx, a_k in a_m:   \n",
    "                    # Row match\n",
    "                    if midx == m:\n",
    "                        #\n",
    "                        # Traverse non-empty element of bottom rank\n",
    "                        #\n",
    "                        for k, a_val in a_k:  \n",
    "\n",
    "                            #print(f\"{a_val} \", end='')\n",
    "\n",
    "                            #\n",
    "                            # Animation bookkeeping\n",
    "                            #\n",
    "                            canvas.addActivity((cy,ch,m,k), spacetime=(0,cycle))\n",
    "                            cycle += 1\n",
    "\n",
    "displayCanvas(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: multiply two C2SR-formatted tensors\n",
    "\n",
    "The following code illustrates the traversal and accesses associated with the computation of untiled matrix multiply as expressed by the following Einsum:\n",
    "\n",
    "$$ Z_{m,n} = A_{m,k} \\times B_{k,n} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Create the input/output tensors\n",
    "#\n",
    "K = getShape(tm2, \"K\")\n",
    "M = getShape(tm2, \"M\")\n",
    "N = getShape(tm2, \"N\")\n",
    "\n",
    "A_MK = tm2.makeTensor(\"A\")\n",
    "B_KN = tm2.makeTensor(\"B\")\n",
    "\n",
    "Z_MN = Tensor(name=\"Z\", rank_ids=[\"M\", \"N\"], shape=[M, N])\n",
    "\n",
    "uncompressTensor(Z_MN)\n",
    "\n",
    "#\n",
    "# Display the tensors\n",
    "#\n",
    "print(\"Problem instance:\")\n",
    "print(f\"M: {M}\")\n",
    "print(f\"K: {K}\")\n",
    "print(f\"N: {N}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Input A\")\n",
    "displayTensor(A_MK)\n",
    "print(\"Input B\")\n",
    "displayTensor(B_KN)\n",
    "print(\"Output Z (initial)\")\n",
    "displayTensor(Z_MN)\n",
    "\n",
    "#\n",
    "# Get the root fibers of each tensor\n",
    "#\n",
    "a_m = A_MK.getRoot()\n",
    "b_k = B_KN.getRoot()\n",
    "z_m = Z_MN.getRoot()\n",
    "\n",
    "#\n",
    "# Animation bookkeeping\n",
    "#\n",
    "canvas = createCanvas(A_MK, B_KN, Z_MN)\n",
    "\n",
    "cycle = 0\n",
    "\n",
    "#\n",
    "# Traverse non-empty elements of top rank of `A`\n",
    "#\n",
    "for m, a_k in a_m:\n",
    "    #\n",
    "    # Traverse the K rank of `A`\n",
    "    #\n",
    "    for k, a_val in a_k:\n",
    "        #\n",
    "        # Obtain the matching fiber in `B`\n",
    "        #\n",
    "        b_n = b_k.getPayload(k)\n",
    "        \n",
    "        #\n",
    "        # Traverse the bottom rank of `B`\n",
    "        #\n",
    "        for n, b_val in b_n:\n",
    "            #\n",
    "            # Do the reduction\n",
    "            #\n",
    "            z_m[m][n] += a_val * b_val\n",
    "            \n",
    "            #\n",
    "            # Animation bookkeeping\n",
    "            #\n",
    "            canvas.addActivity((m,k), (k,n), (m, n), spacetime=(0,cycle))\n",
    "            cycle += 1\n",
    "            \n",
    "#\n",
    "# Display results\n",
    "#\n",
    "print(\"Output tensor Z (final)\")\n",
    "displayTensor(Z_MN)\n",
    "\n",
    "displayCanvas(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: is matraptor dataflow identical to Tiled Matrix Multiply?\n",
    "\n",
    "$$ Z_{n1,m,n0} = A_{m,k} \\times B_{n1,k,n0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting of B\n",
    "\n",
    "First we look at the sequence for splitting the `B` tensor along the N rank.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- The coordinates in the N0 rank are the original N coordinates\n",
    "- We are assuming that this is done offline... Online tiling is beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create tensors\n",
    "#\n",
    "\n",
    "N0 = 3\n",
    "\n",
    "B_KN = tm2.makeTensor(\"B\")\n",
    "\n",
    "print(\"Original B tensor\")\n",
    "displayTensor(B_KN)\n",
    "\n",
    "#\n",
    "# Split the `N` rank of `B` into a series of fibers with shape `N0`\n",
    "# then rename the coordinates of the N1 rank to match the coordinates we want in the output (hacky)\n",
    "#\n",
    "B_N1N0K = B_KN.splitUniform(N0, depth=1)\n",
    "B_N1N0K = B_N1N0K.updateCoords(lambda n, c, p: n, depth=1)\n",
    "\n",
    "print(\"B tensor with N rank split into fibers with a shape={N0}\")\n",
    "displayTensor(B_N1N0K)\n",
    "\n",
    "#\n",
    "# Swizzle the ranks of B so that we can traverse it in a **concordant** fashion\n",
    "#\n",
    "B_N1KN0 = B_N1N0K.swapRanks()\n",
    "\n",
    "print(\"B tensor split and swizzed for concordant traversal\")\n",
    "displayTensor(B_N1KN0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiled Matrix Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Create the input/output tensors\n",
    "#\n",
    "K = getShape(tm2, \"K\")\n",
    "M = getShape(tm2, \"M\")\n",
    "N = getShape(tm2, \"N\")\n",
    "\n",
    "N1 = 2\n",
    "N0 = (N+1)//2\n",
    "\n",
    "#\n",
    "# Create tensors\n",
    "#\n",
    "\n",
    "A_MK = tm2.makeTensor(\"A\")\n",
    "B_KN = tm2.makeTensor(\"B\")\n",
    "\n",
    "\n",
    "B_N1KN0 = B_KN.splitUniform(N0, depth=1).swapRanks()\n",
    "B_N1KN0 = B_N1KN0.updateCoords(lambda n, c, p: n)\n",
    "\n",
    "Z_MN = Tensor(name=\"Z\",\n",
    "              rank_ids=[\"M\", \"N\"],\n",
    "                 shape=[M, N])\n",
    "uncompressTensor(Z_MN)\n",
    "\n",
    "Z_N1MN0 = Z_MN.splitUniform(N0, depth=1).swapRanks()\n",
    "Z_N1MN0 = Z_N1MN0.updateCoords(lambda n, c, p: n)\n",
    "Z_N1MN0.setMutable(True)\n",
    "\n",
    "#\n",
    "# Display Tensors\n",
    "#\n",
    "print(\"Problem instance:\")\n",
    "print(f\"K:  {K}\")\n",
    "print(f\"M:  {M}\")\n",
    "print(f\"N:  {N}\")\n",
    "print(f\"N1: {N1}\")\n",
    "print(f\"N0: {N0}\")\n",
    "\n",
    "displayTensor(A_MK)\n",
    "displayTensor(B_N1KN0)\n",
    "displayTensor(Z_N1MN0)\n",
    "\n",
    "#\n",
    "# Get the root fibers of each tensor\n",
    "#\n",
    "a_m = A_MK.getRoot()\n",
    "b_n1 = B_N1KN0.getRoot()\n",
    "z_n1 = Z_N1MN0.getRoot()\n",
    "\n",
    "#\n",
    "# Animation bookkeeping\n",
    "#\n",
    "canvas = createCanvas(A_MK, B_N1KN0, Z_N1MN0)\n",
    "\n",
    "cycle = 0\n",
    "\n",
    "#\n",
    "# Traverse non-empty elements of `N1` (top) rank of `B`\n",
    "#\n",
    "for n1, b_k in b_n1:\n",
    "    #\n",
    "    # Traverse non-empty elements of `M` (top) rank of A\n",
    "    #\n",
    "    for m, a_k in a_m:\n",
    "        #\n",
    "        # Traverse the K (bottom) rank of `A`\n",
    "        #\n",
    "        for k, a_val in a_k:\n",
    "            #\n",
    "            # Obtain the matching fiber in `B`\n",
    "            #\n",
    "            b_n0 = b_k.getPayload(k)\n",
    "        \n",
    "            #\n",
    "            # Traverse the `N0` (bottom) rank of `B`\n",
    "            #\n",
    "            for n0, b_val in b_n0:\n",
    "                #\n",
    "                # Do the reduction\n",
    "                #\n",
    "                # Note hack to get right position in `N0` rank of `Z`\n",
    "                #\n",
    "                z_n1[n1][m][n0%N0] += a_val * b_val\n",
    "            \n",
    "                #\n",
    "                # Animation bookkeeping\n",
    "                #\n",
    "                canvas.addActivity([(m,k)], [(n1,k,n0)], [(n1, m, n0)], \n",
    "                                   spacetime=(0,cycle))\n",
    "                cycle += 1\n",
    "            \n",
    "displayTensor(Z_N1MN0)\n",
    "displayCanvas(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing area\n",
    "\n",
    "For running alternative algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
